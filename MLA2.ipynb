{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COSC2673 - Machine Learning - Assignment 2\n",
    "### Authors: Louise Platts (S3908818) & Samuel Macintyre (S3888492)\n",
    "\n",
    "# Classifying Images of Road Traffic Signs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Libraries Required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.0\n",
      "['/device:CPU:0', '/device:GPU:0']\n",
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "def get_available_devices():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos]\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "print(get_available_devices())\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statements\n",
    "    \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import glob\n",
    "from PIL import Image\n",
    "import zipfile\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.initializers import glorot_uniform\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Understanding & Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_reader(path):\n",
    "    image_list = []\n",
    "    for filepath in glob.glob(path, recursive=True):\n",
    "        shape = filepath.split(\"\\\\\")[-3]\n",
    "        type = filepath.split(\"\\\\\")[-2]\n",
    "        filename = filepath.split(\"\\\\\")[-1]\n",
    "        label = filename.split(\".\")[0]\n",
    "        \n",
    "        image_list.append({\"filepath\": filepath,'filename':filename,'label':label,'shape':shape,'type':type})\n",
    "\n",
    "    dataset = pd.DataFrame(image_list)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def drop_unused_target(train, val, col):\n",
    "    train1 = train.copy()\n",
    "    val1 = val.copy()\n",
    "\n",
    "    train1 = train1.drop(col,axis=1)\n",
    "    val1 = val1.drop(col,axis=1)\n",
    "    return train1, val1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data : 2959, Val Data: 740\n",
      "Shape Classes:['diamond' 'hex' 'round' 'square' 'triangle']\n",
      "Type Classes: ['rightofway' 'stop' 'bicycle' 'limitedtraffic' 'noentry' 'noparking'\n",
      " 'roundabout' 'speed' 'trafficdirective' 'traveldirection' 'continue'\n",
      " 'crossing' 'laneend' 'parking' 'giveway' 'warning']\n",
      "Num Shape Classes: 5, Num Type Classes: 16\n"
     ]
    }
   ],
   "source": [
    "raw_data = image_reader(\"./trafficsigns_dataset/*/*/*.png\")\n",
    "# raw_data.head()\n",
    "\n",
    "# Split data into train and test\n",
    "train_data, val_data = train_test_split(raw_data, test_size=0.2, random_state=42)\n",
    "\n",
    "TOTAL_SHAPES = raw_data['shape'].nunique()\n",
    "TOTAL_TYPES = raw_data['type'].nunique()\n",
    "\n",
    "SHAPE_NAMES = raw_data['shape'].unique()\n",
    "TYPE_NAMES = raw_data['type'].unique()\n",
    "\n",
    "DIM = 28\n",
    "RES_DIM = 28\n",
    "\n",
    "test_image = Image.open(raw_data['filepath'][0])\n",
    "NUM_BANDS = len(test_image.getbands())\n",
    "\n",
    "print(f\"Train data : {train_data.shape[0]}, Val Data: {val_data.shape[0]}\")\n",
    "print(f\"Shape Classes:{SHAPE_NAMES}\")\n",
    "print(f\"Type Classes: {TYPE_NAMES}\")\n",
    "print(f\"Num Shape Classes: {TOTAL_SHAPES}, Num Type Classes: {TOTAL_TYPES}\")\n",
    "\n",
    "\n",
    "# Split training and validation into frames for respective classification type\n",
    "shape_train_data, shape_val_data = drop_unused_target(train_data, val_data, \"type\")\n",
    "type_train_data, type_val_data = drop_unused_target(train_data, val_data, \"shape\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'image_path'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\platt\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3621\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/platt/anaconda3/envs/tf_gpu/lib/site-packages/pandas/core/indexes/base.py?line=3619'>3620</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> <a href='file:///c%3A/Users/platt/anaconda3/envs/tf_gpu/lib/site-packages/pandas/core/indexes/base.py?line=3620'>3621</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[0;32m   <a href='file:///c%3A/Users/platt/anaconda3/envs/tf_gpu/lib/site-packages/pandas/core/indexes/base.py?line=3621'>3622</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\platt\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\pandas\\_libs\\index.pyx:136\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\platt\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\pandas\\_libs\\index.pyx:163\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5198\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5206\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'image_path'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\platt\\OneDrive\\Documents\\RMIT\\Machine Learning\\Assignment 2\\windows\\the_village_idiots\\MLA2.ipynb Cell 9'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/platt/OneDrive/Documents/RMIT/Machine%20Learning/Assignment%202/windows/the_village_idiots/MLA2.ipynb#ch0000079?line=0'>1</a>\u001b[0m \u001b[39m# Show some random images for context\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/platt/OneDrive/Documents/RMIT/Machine%20Learning/Assignment%202/windows/the_village_idiots/MLA2.ipynb#ch0000079?line=1'>2</a>\u001b[0m r_inx \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mchoice(\u001b[39m100\u001b[39m, \u001b[39m4\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/platt/OneDrive/Documents/RMIT/Machine%20Learning/Assignment%202/windows/the_village_idiots/MLA2.ipynb#ch0000079?line=2'>3</a>\u001b[0m rand_data \u001b[39m=\u001b[39m train_data\u001b[39m.\u001b[39;49mloc[r_inx,\u001b[39m\"\u001b[39;49m\u001b[39mimage_path\u001b[39;49m\u001b[39m\"\u001b[39;49m]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/platt/OneDrive/Documents/RMIT/Machine%20Learning/Assignment%202/windows/the_village_idiots/MLA2.ipynb#ch0000079?line=4'>5</a>\u001b[0m plt\u001b[39m.\u001b[39mfigure(figsize\u001b[39m=\u001b[39m(\u001b[39m16\u001b[39m,\u001b[39m4\u001b[39m))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/platt/OneDrive/Documents/RMIT/Machine%20Learning/Assignment%202/windows/the_village_idiots/MLA2.ipynb#ch0000079?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, image_path \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(rand_data):\n",
      "File \u001b[1;32mc:\\Users\\platt\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\pandas\\core\\indexing.py:961\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/platt/anaconda3/envs/tf_gpu/lib/site-packages/pandas/core/indexing.py?line=958'>959</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_scalar_access(key):\n\u001b[0;32m    <a href='file:///c%3A/Users/platt/anaconda3/envs/tf_gpu/lib/site-packages/pandas/core/indexing.py?line=959'>960</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39m_get_value(\u001b[39m*\u001b[39mkey, takeable\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_takeable)\n\u001b[1;32m--> <a href='file:///c%3A/Users/platt/anaconda3/envs/tf_gpu/lib/site-packages/pandas/core/indexing.py?line=960'>961</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem_tuple(key)\n\u001b[0;32m    <a href='file:///c%3A/Users/platt/anaconda3/envs/tf_gpu/lib/site-packages/pandas/core/indexing.py?line=961'>962</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/platt/anaconda3/envs/tf_gpu/lib/site-packages/pandas/core/indexing.py?line=962'>963</a>\u001b[0m     \u001b[39m# we by definition only have the 0th axis\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/platt/anaconda3/envs/tf_gpu/lib/site-packages/pandas/core/indexing.py?line=963'>964</a>\u001b[0m     axis \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxis \u001b[39mor\u001b[39;00m \u001b[39m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\platt\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\pandas\\core\\indexing.py:1140\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_tuple\u001b[1;34m(self, tup)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/platt/anaconda3/envs/tf_gpu/lib/site-packages/pandas/core/indexing.py?line=1137'>1138</a>\u001b[0m \u001b[39mwith\u001b[39;00m suppress(IndexingError):\n\u001b[0;32m   <a href='file:///c%3A/Users/platt/anaconda3/envs/tf_gpu/lib/site-packages/pandas/core/indexing.py?line=1138'>1139</a>\u001b[0m     tup \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_ellipsis(tup)\n\u001b[1;32m-> <a href='file:///c%3A/Users/platt/anaconda3/envs/tf_gpu/lib/site-packages/pandas/core/indexing.py?line=1139'>1140</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem_lowerdim(tup)\n\u001b[0;32m   <a href='file:///c%3A/Users/platt/anaconda3/envs/tf_gpu/lib/site-packages/pandas/core/indexing.py?line=1141'>1142</a>\u001b[0m \u001b[39m# no multi-index, so validate all of the indexers\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/platt/anaconda3/envs/tf_gpu/lib/site-packages/pandas/core/indexing.py?line=1142'>1143</a>\u001b[0m tup \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_tuple_indexer(tup)\n",
      "File \u001b[1;32mc:\\Users\\platt\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\pandas\\core\\indexing.py:867\u001b[0m, in \u001b[0;36m_LocationIndexer._getitem_lowerdim\u001b[1;34m(self, tup)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/platt/anaconda3/envs/tf_gpu/lib/site-packages/pandas/core/indexing.py?line=862'>863</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, key \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(tup):\n\u001b[0;32m    <a href='file:///c%3A/Users/platt/anaconda3/envs/tf_gpu/lib/site-packages/pandas/core/indexing.py?line=863'>864</a>\u001b[0m     \u001b[39mif\u001b[39;00m is_label_like(key):\n\u001b[0;32m    <a href='file:///c%3A/Users/platt/anaconda3/envs/tf_gpu/lib/site-packages/pandas/core/indexing.py?line=864'>865</a>\u001b[0m         \u001b[39m# We don't need to check for tuples here because those are\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/platt/anaconda3/envs/tf_gpu/lib/site-packages/pandas/core/indexing.py?line=865'>866</a>\u001b[0m         \u001b[39m#  caught by the _is_nested_tuple_indexer check above.\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/platt/anaconda3/envs/tf_gpu/lib/site-packages/pandas/core/indexing.py?line=866'>867</a>\u001b[0m         section \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem_axis(key, axis\u001b[39m=\u001b[39;49mi)\n\u001b[0;32m    <a href='file:///c%3A/Users/platt/anaconda3/envs/tf_gpu/lib/site-packages/pandas/core/indexing.py?line=868'>869</a>\u001b[0m         \u001b[39m# We should never have a scalar section here, because\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/platt/anaconda3/envs/tf_gpu/lib/site-packages/pandas/core/indexing.py?line=869'>870</a>\u001b[0m         \u001b[39m#  _getitem_lowerdim is only called after a check for\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/platt/anaconda3/envs/tf_gpu/lib/site-packages/pandas/core/indexing.py?line=870'>871</a>\u001b[0m         \u001b[39m#  is_scalar_access, which that would be.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/platt/anaconda3/envs/tf_gpu/lib/site-packages/pandas/core/indexing.py?line=871'>872</a>\u001b[0m         \u001b[39mif\u001b[39;00m section\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mndim:\n\u001b[0;32m    <a href='file:///c%3A/Users/platt/anaconda3/envs/tf_gpu/lib/site-packages/pandas/core/indexing.py?line=872'>873</a>\u001b[0m             \u001b[39m# we're in the middle of slicing through a MultiIndex\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/platt/anaconda3/envs/tf_gpu/lib/site-packages/pandas/core/indexing.py?line=873'>874</a>\u001b[0m             \u001b[39m# revise the key wrt to `section` by inserting an _NS\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\platt\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\pandas\\core\\indexing.py:1202\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/platt/anaconda3/envs/tf_gpu/lib/site-packages/pandas/core/indexing.py?line=1199'>1200</a>\u001b[0m \u001b[39m# fall thru to straight lookup\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/platt/anaconda3/envs/tf_gpu/lib/site-packages/pandas/core/indexing.py?line=1200'>1201</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_key(key, axis)\n\u001b[1;32m-> <a href='file:///c%3A/Users/platt/anaconda3/envs/tf_gpu/lib/site-packages/pandas/core/indexing.py?line=1201'>1202</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_label(key, axis\u001b[39m=\u001b[39;49maxis)\n",
      "File \u001b[1;32mc:\\Users\\platt\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\pandas\\core\\indexing.py:1153\u001b[0m, in \u001b[0;36m_LocIndexer._get_label\u001b[1;34m(self, label, axis)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/platt/anaconda3/envs/tf_gpu/lib/site-packages/pandas/core/indexing.py?line=1150'>1151</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_label\u001b[39m(\u001b[39mself\u001b[39m, label, axis: \u001b[39mint\u001b[39m):\n\u001b[0;32m   <a href='file:///c%3A/Users/platt/anaconda3/envs/tf_gpu/lib/site-packages/pandas/core/indexing.py?line=1151'>1152</a>\u001b[0m     \u001b[39m# GH#5667 this will fail if the label is not present in the axis.\u001b[39;00m\n\u001b[1;32m-> <a href='file:///c%3A/Users/platt/anaconda3/envs/tf_gpu/lib/site-packages/pandas/core/indexing.py?line=1152'>1153</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mobj\u001b[39m.\u001b[39;49mxs(label, axis\u001b[39m=\u001b[39;49maxis)\n",
      "File \u001b[1;32mc:\\Users\\platt\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\pandas\\core\\generic.py:3849\u001b[0m, in \u001b[0;36mNDFrame.xs\u001b[1;34m(self, key, axis, level, drop_level)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/platt/anaconda3/envs/tf_gpu/lib/site-packages/pandas/core/generic.py?line=3846'>3847</a>\u001b[0m \u001b[39mif\u001b[39;00m axis \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   <a href='file:///c%3A/Users/platt/anaconda3/envs/tf_gpu/lib/site-packages/pandas/core/generic.py?line=3847'>3848</a>\u001b[0m     \u001b[39mif\u001b[39;00m drop_level:\n\u001b[1;32m-> <a href='file:///c%3A/Users/platt/anaconda3/envs/tf_gpu/lib/site-packages/pandas/core/generic.py?line=3848'>3849</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m[key]\n\u001b[0;32m   <a href='file:///c%3A/Users/platt/anaconda3/envs/tf_gpu/lib/site-packages/pandas/core/generic.py?line=3849'>3850</a>\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\n\u001b[0;32m   <a href='file:///c%3A/Users/platt/anaconda3/envs/tf_gpu/lib/site-packages/pandas/core/generic.py?line=3850'>3851</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\platt\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\pandas\\core\\frame.py:3505\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/platt/anaconda3/envs/tf_gpu/lib/site-packages/pandas/core/frame.py?line=3502'>3503</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   <a href='file:///c%3A/Users/platt/anaconda3/envs/tf_gpu/lib/site-packages/pandas/core/frame.py?line=3503'>3504</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> <a href='file:///c%3A/Users/platt/anaconda3/envs/tf_gpu/lib/site-packages/pandas/core/frame.py?line=3504'>3505</a>\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[0;32m   <a href='file:///c%3A/Users/platt/anaconda3/envs/tf_gpu/lib/site-packages/pandas/core/frame.py?line=3505'>3506</a>\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   <a href='file:///c%3A/Users/platt/anaconda3/envs/tf_gpu/lib/site-packages/pandas/core/frame.py?line=3506'>3507</a>\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\platt\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3623\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/platt/anaconda3/envs/tf_gpu/lib/site-packages/pandas/core/indexes/base.py?line=3620'>3621</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   <a href='file:///c%3A/Users/platt/anaconda3/envs/tf_gpu/lib/site-packages/pandas/core/indexes/base.py?line=3621'>3622</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m-> <a href='file:///c%3A/Users/platt/anaconda3/envs/tf_gpu/lib/site-packages/pandas/core/indexes/base.py?line=3622'>3623</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/platt/anaconda3/envs/tf_gpu/lib/site-packages/pandas/core/indexes/base.py?line=3623'>3624</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m   <a href='file:///c%3A/Users/platt/anaconda3/envs/tf_gpu/lib/site-packages/pandas/core/indexes/base.py?line=3624'>3625</a>\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/platt/anaconda3/envs/tf_gpu/lib/site-packages/pandas/core/indexes/base.py?line=3625'>3626</a>\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/platt/anaconda3/envs/tf_gpu/lib/site-packages/pandas/core/indexes/base.py?line=3626'>3627</a>\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/platt/anaconda3/envs/tf_gpu/lib/site-packages/pandas/core/indexes/base.py?line=3627'>3628</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'image_path'"
     ]
    }
   ],
   "source": [
    "# Show some random images for context\n",
    "r_inx = np.random.choice(100, 4)\n",
    "rand_data = train_data.loc[r_inx,\"image_path\"]\n",
    "\n",
    "plt.figure(figsize=(16,4))\n",
    "for i, image_path in enumerate(rand_data):\n",
    "    im = np.asarray(Image.open(image_path))\n",
    "    plt.subplot(1,4,i+1)\n",
    "    plt.imshow(im,cmap='gray')\n",
    "    plt.axis('off')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_subplot(x,data,ax,order,title):\n",
    "    fontsize = 12\n",
    "    sns.countplot(x=x,data=data, order=order, ax=ax)\n",
    "    ax.set_title(title, fontsize=fontsize)\n",
    "    ax.set_xlabel(x, fontsize=fontsize)\n",
    "    ax.set_ylabel(\"Count\", fontsize=fontsize)\n",
    "    ax.tick_params(labelsize=fontsize)\n",
    "    ax.tick_params(axis=\"x\",rotation=90)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Distribution of classes in training data splits compared to raw\n",
    "\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "\n",
    "fig, ax = plt.subplots(2,3, figsize=(30,18))\n",
    "\n",
    "make_subplot(x=\"shape\",data=raw_data, order=SHAPE_NAMES, ax=ax[0,0], title=\"Shape Count Raw Data\")\n",
    "make_subplot(x=\"shape\",data=shape_train_data, order=SHAPE_NAMES, ax=ax[0,1], title=\"Shape Count Training Data\")\n",
    "make_subplot(x=\"shape\",data=shape_val_data, order=SHAPE_NAMES, ax=ax[0,2], title=\"Shape Count Validation Data\")\n",
    "make_subplot(x=\"type\",data=raw_data, order=TYPE_NAMES, ax=ax[1,0], title=\"Type Count Raw Data\")\n",
    "make_subplot(x=\"type\",data=type_train_data, order=TYPE_NAMES, ax=ax[1,1], title=\"Type Count Training Data\")\n",
    "make_subplot(x=\"type\",data=type_train_data, order=TYPE_NAMES, ax=ax[1,2], title=\"Type Count Validation Data\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModel:\n",
    "\n",
    "    def __init__(self, epochs=100, activation=\"relu\"):\n",
    "        self.epochs = epochs\n",
    "        self.activation = activation\n",
    "\n",
    "    def create_data_generator(self, df,datagen,batch_size,target,target_size=(28,28)):\n",
    "        print(target_size)\n",
    "        generator = datagen.flow_from_dataframe(\n",
    "            dataframe=df,\n",
    "            directory = './',\n",
    "            x_col='filepath',\n",
    "            y_col=target,\n",
    "            target_size=target_size,\n",
    "            batch_size=batch_size,\n",
    "            color_mode=\"grayscale\",\n",
    "            class_mode='categorical',\n",
    "            rotation_range=45,\n",
    "            width_shift_range=0.2,\n",
    "            height_shift_range=0.2,\n",
    "            brightness_range=[0.75,1.25],\n",
    "            random_zoom=[0.75,1.25],\n",
    "            horizontal_flip=True,\n",
    "            vertical_flip=True\n",
    "        )\n",
    "\n",
    "        return generator\n",
    "\n",
    "    def build_vgg_model(self, num_classes):\n",
    "\n",
    "        self.model = tf.keras.Sequential([\n",
    "            #VGG block 1\n",
    "            tf.keras.layers.Conv2D(DIM, (3, 3), activation=self.activation, padding='same', input_shape=(DIM, DIM, NUM_BANDS)),\n",
    "            tf.keras.layers.Conv2D(DIM, (3, 3), activation=self.activation, padding='same'),\n",
    "            tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "            \n",
    "            #VGG block 2\n",
    "            tf.keras.layers.Conv2D(DIM*2, (3, 3), activation=self.activation, padding='same'),\n",
    "            tf.keras.layers.Conv2D(DIM*2, (3, 3), activation=self.activation, padding='same'),\n",
    "            tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "            \n",
    "            #VGG block 3\n",
    "            tf.keras.layers.Conv2D(DIM*4, (3, 3), activation=self.activation, padding='same'),\n",
    "            tf.keras.layers.Conv2D(DIM*4, (3, 3), activation=self.activation, padding='same'),\n",
    "            tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "            \n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(DIM*4, activation=self.activation),\n",
    "            tf.keras.layers.Dense(num_classes)\n",
    "        ])\n",
    "\n",
    "    def build_resnet_model(self,num_classes):\n",
    "        self.model = ResNetModified(num_classes=num_classes, activation=self.activation, input_shape=(RES_DIM,RES_DIM,NUM_BANDS))\n",
    "\n",
    "    def compile_resnet_model(self):\n",
    "        self.model.compile(optimizer=\"adam\", \n",
    "            loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False), \n",
    "            metrics=[\"categorical_accuracy\"])\n",
    "    \n",
    "    def compile_vgg_model(self):\n",
    "        self.model.compile(optimizer='adam',\n",
    "            loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "            metrics=['categorical_accuracy'])\n",
    "\n",
    "    def fit_model(self):\n",
    "       return self.model.fit_generator(self.train_generator, validation_data=self.val_generator,\\\n",
    "           epochs=self.epochs,verbose=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model\n",
    "VGG Baseline Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_train_datagen = ImageDataGenerator(rescale=1.0/255, data_format='channels_last')\n",
    "shape_val_datagen = ImageDataGenerator(rescale=1.0/255, data_format='channels_last')\n",
    "\n",
    "type_train_datagen = ImageDataGenerator(rescale=1.0/255, data_format='channels_last')\n",
    "type_val_datagen = ImageDataGenerator(rescale=1.0/255, data_format='channels_last')\n",
    "\n",
    "# RESNET PAPER USED Batch Size DIM*4. Batchsize to be updated later (Impact on Run Time)\n",
    "vgg_batch_size = 32\n",
    "\n",
    "\n",
    "vgg_shape_model = CNNModel()\n",
    "vgg_type_model = CNNModel()\n",
    "\n",
    "\n",
    "# Baseline VGG Shape Model Data Generators\n",
    "vgg_shape_model.train_generator = vgg_shape_model.create_data_generator(\\\n",
    "    shape_train_data,shape_train_datagen,vgg_batch_size,\"shape\")\n",
    "\n",
    "vgg_shape_model.val_generator = vgg_shape_model.create_data_generator(\\\n",
    "    shape_val_data,shape_val_datagen,vgg_batch_size,\"shape\")\n",
    "\n",
    "# Baseline VGG Type Model Data Generators\n",
    "vgg_type_model.train_generator = vgg_type_model.create_data_generator(\\\n",
    "    type_train_data,type_train_datagen,vgg_batch_size,\"type\")\n",
    "\n",
    "vgg_type_model.val_generator = vgg_type_model.create_data_generator(\\\n",
    "    type_val_data,type_val_datagen,vgg_batch_size,\"type\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Model\n",
    "vgg_shape_model.build_vgg_model(TOTAL_SHAPES)\n",
    "vgg_type_model.build_vgg_model(TOTAL_TYPES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_shape_model.compile_vgg_model()\n",
    "vgg_type_model.compile_vgg_model()\n",
    "\n",
    "vgg_shape_fit = vgg_shape_model.fit_model()\n",
    "vgg_type_fit = vgg_type_model.fit_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(train_loss, val_loss, train_metric, val_metric, metric_name='Accuracy', title=None):\n",
    "    plt.figure(figsize=(10,5))\n",
    "    \n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(train_loss, 'r--')\n",
    "    plt.plot(val_loss, 'b--')\n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend(['train', 'val'], loc='upper left')\n",
    "    plt.title(title)\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(train_metric, 'r--')\n",
    "    plt.plot(val_metric, 'b--')\n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.ylabel(metric_name)\n",
    "    plt.legend(['train', 'val'], loc='upper left')\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curve(vgg_shape_fit.history['loss'], vgg_shape_fit.history['val_loss'], \n",
    "                    vgg_shape_fit.history['categorical_accuracy'], vgg_shape_fit.history['val_categorical_accuracy'], \n",
    "                    metric_name='Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curve(vgg_type_fit.history['loss'], vgg_type_fit.history['val_loss'], \n",
    "                    vgg_type_fit.history['categorical_accuracy'], vgg_type_fit.history['val_categorical_accuracy'], \n",
    "                    metric_name='Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resnet Model\n",
    "https://github.com/priya-dwivedi/Deep-Learning/blob/master/resnet_keras/Residual_Networks_yourself.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identity block -> standard block used in ResNets \n",
    "def identity_block(X, activation, f, filters, stage, block):\n",
    "    '''\n",
    "    identity blocks are required to transform into a residual network \n",
    "    inputs:\n",
    "    X -> input tensor of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    f -> integer determining the shape of the middle CONV's window\n",
    "    filters -> list of integers, defines number of filters in the CONV layer \n",
    "    stage -> integer, names the layers depending on position in NN\n",
    "    block -> string, used to name the layers, depending on their position in the network\n",
    "\n",
    "    outputs:\n",
    "    X -> the identity block, tensor of shape (n_H, n_W, n_C)\n",
    "    '''\n",
    "    # defines the name of branch according to current position\n",
    "    conv_name_base = \"res\" + str(stage) + block + \"_branch\"\n",
    "    bn_name_base = \"bn\" + str(stage) + block + \"_branch\"\n",
    "\n",
    "    # retrieve the filters to apply\n",
    "    F1, F2, F3 = filters\n",
    "\n",
    "    # save current input tensor\n",
    "    X_save = X \n",
    "\n",
    "    # first component \n",
    "    X = Conv2D(filters=F1, kernel_size=(1,1), strides=(1,1), padding=\"valid\", name=conv_name_base+\"2a\", kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    # normalises the channels axis\n",
    "    X = BatchNormalization(axis=-1, name=bn_name_base+\"2a\")(X)\n",
    "    X = Activation(activation)(X)\n",
    "\n",
    "    # second component\n",
    "    X = Conv2D(filters=F2, kernel_size=(f,f), strides=(1,1), padding=\"same\", name=conv_name_base+\"2b\", kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis=-1, name=bn_name_base+\"2b\")(X)\n",
    "    X = Activation(activation)(X)\n",
    "\n",
    "    # third component\n",
    "    X = Conv2D(filters=F3, kernel_size=(1,1), strides=(1,1), padding=\"valid\", name=conv_name_base+\"2c\", kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis=-1, name=bn_name_base+\"2c\")(X)\n",
    "\n",
    "    # add shorcut connection to main path \n",
    "    X = Add()([X, X_save])\n",
    "    X = Activation(activation)(X)\n",
    "\n",
    "    return X\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convolutional block -> standard block used in ResNets to ensure input and ouptut dimension\n",
    "def convolutional_block(X, activation, f, filters, stage, block, s=2):\n",
    "    '''\n",
    "    identity blocks are required to transform into a residual network \n",
    "    inputs:\n",
    "    X -> input tensor of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    f -> integer determining the shape of the middle CONV's window\n",
    "    filters -> list of integers, defines number of filters in the CONV layer \n",
    "    stage -> integer, names the layers depending on position in NN\n",
    "    block -> string, used to name the layers, depending on their position in the network\n",
    "    s -> integer, specifies the stride to be used, i.e. to reduce the dimension by a factor of 2, use a stride of 2\n",
    "\n",
    "    outputs:\n",
    "    X -> the identity block, tensor of shape (n_H, n_W, n_C)\n",
    "    '''\n",
    "    # defines the name of branch according to current position\n",
    "    conv_name_base = \"res\" + str(stage) + block + \"_branch\"\n",
    "    bn_name_base = \"bn\" + str(stage) + block + \"_branch\"\n",
    "\n",
    "    # retrieve the filters to apply\n",
    "    F1, F2, F3 = filters\n",
    "\n",
    "    # to build shortcut path\n",
    "    X_shortcut = X \n",
    "\n",
    "    # first component \n",
    "    X = Conv2D(filters=F1, kernel_size=(1,1), strides=(s,s), padding=\"valid\", name=conv_name_base+\"2a\", kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    # normalises the channels axis\n",
    "    X = BatchNormalization(axis=-1, name=bn_name_base+\"2a\")(X)\n",
    "    X = Activation(activation)(X)\n",
    "\n",
    "    # second component\n",
    "    X = Conv2D(filters=F2, kernel_size=(f,f), strides=(1,1), padding=\"same\", name=conv_name_base+\"2b\", kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis=-1, name=bn_name_base+\"2b\")(X)\n",
    "    X = Activation(activation)(X)\n",
    "\n",
    "    # third component\n",
    "    X = Conv2D(filters=F3, kernel_size=(1,1), strides=(1,1), padding=\"valid\", name=conv_name_base+\"2c\", kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis=-1, name=bn_name_base+\"2c\")(X)\n",
    "\n",
    "    # build the shortcut path \n",
    "    X_shortcut = Conv2D(filters=F3, kernel_size=(1,1), strides=(s,s), padding=\"valid\", name=conv_name_base+\"1\", kernel_initializer=glorot_uniform(seed=0))(X_shortcut)\n",
    "    X_shortcut = BatchNormalization(axis=-1, name=bn_name_base+\"1\")(X_shortcut)\n",
    "\n",
    "\n",
    "    # add shortcut connection to main path \n",
    "    X = Add()([X, X_shortcut])\n",
    "    X = Activation(activation)(X)\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modified version of 50 layer ResNet Model\n",
    "def ResNetModified(num_classes, activation, input_shape=(DIM,DIM,NUM_BANDS)):\n",
    "    '''\n",
    "    ResNetModified Architecture follows:\n",
    "    (stage 1) = CONV2D -> BN -> RELU -> MAXPOOL -> \n",
    "    stage 2 = CONVBLOCK -> IDBLOCK x 2 -> \n",
    "    stage 3 = CONVBLOCK -> IDBLOCK X 3 -> \n",
    "    stage 4 = CONVBLOCK -> IDBLOCK X 5 -> \n",
    "    # stage 5 = CONVBLOCK -> IDBLOCK x 2 -> \n",
    "    (stage 6) = AVGPOOL -> TOP LAYER\n",
    "\n",
    "    inputs:\n",
    "    input_shape -> shape of images in dataset \n",
    "    classes -> number of classes to predict\n",
    "\n",
    "    outputs:\n",
    "    model -> a Keras Model() instance for training\n",
    "    '''\n",
    "    # set the input shape\n",
    "    X_input = Input(input_shape)\n",
    "\n",
    "    # initialise the zero padding\n",
    "    X = ZeroPadding2D((3,3))(X_input)\n",
    "\n",
    "    # implement the architecture\n",
    "    X = Conv2D(DIM, (7,7), strides=(2,2), name=\"conv1\", kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis=-1, name=\"bn_conv1\")(X)\n",
    "    X = Activation(activation)(X)\n",
    "    X = MaxPooling2D((3,3), strides=(2,2))(X)\n",
    "\n",
    "    X = convolutional_block(X, activation, f=3, filters=[DIM,DIM,DIM*4], stage=2, block=\"a\", s=1)\n",
    "    X = identity_block(X, activation, 3, [DIM,DIM,DIM*4], stage=2, block=\"b\")\n",
    "    X = identity_block(X, activation, 3, [DIM,DIM,DIM*4], stage=2, block=\"c\")\n",
    "\n",
    "    X = convolutional_block(X, activation, f=3, filters=[DIM*2,DIM*2,DIM*8], stage=3, block=\"a\", s=2)\n",
    "    X = identity_block(X, activation, 3, [DIM*2,DIM*2,DIM*8], stage=3, block=\"b\")\n",
    "    X = identity_block(X, activation, 3, [DIM*2,DIM*2,DIM*8], stage=3, block=\"c\")\n",
    "    X = identity_block(X, activation, 3, [DIM*2,DIM*2,DIM*8], stage=3, block=\"d\")\n",
    "\n",
    "    X = convolutional_block(X, activation, f=3, filters=[DIM*4,DIM*4,DIM*16], stage=4, block=\"a\", s=2)\n",
    "    X = identity_block(X, activation, 3, [DIM*4,DIM*4,DIM*16], stage=4, block=\"b\")\n",
    "    X = identity_block(X, activation, 3, [DIM*4,DIM*4,DIM*16], stage=4, block=\"c\")\n",
    "    X = identity_block(X, activation, 3, [DIM*4,DIM*4,DIM*16], stage=4, block=\"d\")\n",
    "    X = identity_block(X, activation, 3, [DIM*4,DIM*4,DIM*16], stage=4, block=\"e\")\n",
    "    X = identity_block(X, activation, 3, [DIM*4,DIM*4,DIM*16], stage=4, block=\"f\")\n",
    "\n",
    "    # X = convolutional_block(X, f=3, filters=[DIM*8,DIM*8,DIM*32], stage=5, block=\"a\", s=2)\n",
    "    # X = identity_block(X, 3, [DIM*8,DIM*8,DIM*32], stage=5, block=\"b\")\n",
    "    # X = identity_block(X, 3, [DIM*8,DIM*8,DIM*32], stage=5, block=\"c\")\n",
    "\n",
    "    X = AveragePooling2D((2,2), name=\"avg_pool\")(X)\n",
    "    X = Flatten()(X)\n",
    "    X = Dense(num_classes, activation=\"softmax\", name=\"fc\"+str(num_classes), kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "\n",
    "    # create the resnet model\n",
    "    model = Model(inputs=X_input, outputs=X, name=\"ResNetModified\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_batch_size = 32\n",
    "resnet_target_size=(RES_DIM,RES_DIM)\n",
    "\n",
    "resnet_shape_model = CNNModel()\n",
    "resnet_type_model = CNNModel()\n",
    "\n",
    "\n",
    "# Baseline ResNetModified Shape Model Data Generators\n",
    "resnet_shape_model.train_generator = resnet_shape_model.create_data_generator(\\\n",
    "    shape_train_data,shape_train_datagen,resnet_batch_size,\"shape\",target_size=resnet_target_size)\n",
    "\n",
    "resnet_shape_model.val_generator = resnet_shape_model.create_data_generator(\\\n",
    "    shape_val_data,shape_val_datagen,resnet_batch_size,\"shape\",target_size=resnet_target_size)\n",
    "\n",
    "# Baseline ResNetModified Type Model Data Generators\n",
    "resnet_type_model.train_generator = resnet_type_model.create_data_generator(\\\n",
    "    type_train_data,type_train_datagen,resnet_batch_size,\"type\",target_size=resnet_target_size)\n",
    "\n",
    "resnet_type_model.val_generator = resnet_type_model.create_data_generator(\\\n",
    "    type_val_data,type_val_datagen,resnet_batch_size,\"type\",target_size=resnet_target_size)\n",
    "\n",
    "\n",
    "# Build Model\n",
    "resnet_shape_model.build_resnet_model(TOTAL_SHAPES)\n",
    "resnet_type_model.build_resnet_model(TOTAL_TYPES)\n",
    "\n",
    "# Compile Model\n",
    "resnet_shape_model.compile_resnet_model()\n",
    "resnet_type_model.compile_resnet_model()\n",
    "\n",
    "# Fit Model\n",
    "resnet_shape_fit = resnet_shape_model.fit_model()\n",
    "resnet_type_fit = resnet_type_model.fit_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curve(resnet_shape_fit.history['loss'], resnet_shape_fit.history['val_loss'], \n",
    "                    resnet_shape_fit.history['categorical_accuracy'], resnet_shape_fit.history['val_categorical_accuracy'], \n",
    "                    metric_name='Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curve(resnet_type_fit.history['loss'], resnet_type_fit.history['val_loss'], \n",
    "                    resnet_type_fit.history['categorical_accuracy'], resnet_type_fit.history['val_categorical_accuracy'], \n",
    "                    metric_name='Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.keras.utils.plot_model(resnet_shape_model.model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper-Parameter Tuning - VGG Type as better than initial ResNet Type model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "# want to tune batch size, epochs\n",
    "parameters = {\"batch_size\": [10, 20, 40, 60, 80, 100], \"epochs\" : [10, 50, 100]}\n",
    "\n",
    "# save results of tuning for later accessing \n",
    "vgg_type_saved_results = {}\n",
    "attempt = 0\n",
    "\n",
    "# use parameter grid to generate permutations \n",
    "model_parameters = list(ParameterGrid(parameters))\n",
    "\n",
    "for each in model_parameters:\n",
    "    # need to tune batch size, so we initalise and build model + generator according to batch size\n",
    "    type_train_datagen = ImageDataGenerator(rescale=1.0/255, data_format='channels_last')\n",
    "    type_val_datagen = ImageDataGenerator(rescale=1.0/255, data_format='channels_last')\n",
    "\n",
    "    vgg_type_model = CNNModel()\n",
    "\n",
    "    # Baseline VGG Type Model Data Generators\n",
    "    vgg_type_model.train_generator = vgg_type_model.create_data_generator(\\\n",
    "        type_train_data,type_train_datagen, each[\"batch_size\"],\"type\")\n",
    "\n",
    "    vgg_type_model.val_generator = vgg_type_model.create_data_generator(\\\n",
    "        type_val_data,type_val_datagen,each[\"batch_size\"],\"type\")\n",
    "\n",
    "    # build and compile the model for tuning \n",
    "    vgg_type_model.build_vgg_model(TOTAL_TYPES)\n",
    "    vgg_type_model.compile_vgg_model()\n",
    "\n",
    "    # fit the model \n",
    "    vgg_type_fit = vgg_type_model.model.fit_generator(vgg_type_model.train_generator, validation_data=vgg_type_model.val_generator,\\\n",
    "           epochs=each[\"epochs\"],verbose=1)\n",
    "    \n",
    "    vgg_type_saved_results[attempt] = {\"batch\":each[\"batch_size\"], \"epoch\":each[\"epochs\"], \"best_val_accuracy\":max(vgg_type_fit.history[\"val_categorical_accuracy\"]), \\\n",
    "        \"loss\":vgg_type_fit.history['loss'], \"val_loss\":vgg_type_fit.history['val_loss'], \\\n",
    "        \"accuracy\":vgg_type_fit.history['categorical_accuracy'], \"val_accuracy\":vgg_type_fit.history['val_categorical_accuracy']}\n",
    "    attempt += 1\n",
    "    print(\"For batch size {} and epochs of {}, performance:\".format(each[\"batch_size\"], each[\"epochs\"]))\n",
    "    plot_learning_curve(vgg_type_fit.history['loss'], vgg_type_fit.history['val_loss'], \n",
    "                    vgg_type_fit.history['categorical_accuracy'], vgg_type_fit.history['val_categorical_accuracy'], \n",
    "                    metric_name='Accuracy')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_type_saved_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(vgg_type_saved_results)):\n",
    "    plot_title = \"Batch Size: {} Epochs: {} VGG\".format(vgg_type_saved_results[i][\"batch\"], vgg_type_saved_results[i][\"epoch\"])\n",
    "    plot_learning_curve(vgg_type_saved_results[i][\"loss\"], vgg_type_saved_results[i][\"val_loss\"], \n",
    "                    vgg_type_saved_results[i][\"accuracy\"], vgg_type_saved_results[i][\"val_accuracy\"], \n",
    "                    metric_name='Accuracy', title=plot_title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper Parameter Tuning - ResNet Shape as better than initial VGG Shape model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# want to tune batch size, epochs\n",
    "parameters = {\"batch_size\": [10, 20, 40, 60, 80, 100], \"epochs\" : [10, 50, 100]}\n",
    "\n",
    "# save results of tuning for later accessing \n",
    "resnet_shape_saved_results = {}\n",
    "attempt = 0\n",
    "\n",
    "# use parameter grid to generate permutations \n",
    "model_parameters = list(ParameterGrid(parameters))\n",
    "\n",
    "for each in model_parameters:\n",
    "    # need to tune batch size, so we initalise and build model + generator according to batch size\n",
    "    shape_train_datagen = ImageDataGenerator(rescale=1.0/255, data_format='channels_last')\n",
    "    shape_val_datagen = ImageDataGenerator(rescale=1.0/255, data_format='channels_last')\n",
    "\n",
    "    resnet_shape_model = CNNModel()\n",
    "\n",
    "    # Baseline ResNet Shape Model Data Generators\n",
    "    resnet_shape_model.train_generator = resnet_shape_model.create_data_generator(\\\n",
    "        shape_train_data,shape_train_datagen, each[\"batch_size\"],\"shape\")\n",
    "\n",
    "    resnet_shape_model.val_generator = resnet_shape_model.create_data_generator(\\\n",
    "        shape_val_data,shape_val_datagen,each[\"batch_size\"],\"shape\")\n",
    "\n",
    "    # build and compile the model for tuning \n",
    "    resnet_shape_model.build_resnet_model(TOTAL_SHAPES)\n",
    "    resnet_shape_model.compile_resnet_model()\n",
    "\n",
    "    # fit the model \n",
    "    resnet_shape_fit = resnet_shape_model.model.fit_generator(resnet_shape_model.train_generator, validation_data=resnet_shape_model.val_generator,\\\n",
    "           epochs=each[\"epochs\"],verbose=1)\n",
    "    \n",
    "    resnet_shape_saved_results[attempt] = {\"batch\":each[\"batch_size\"], \"epoch\":each[\"epochs\"], \"best_val_accuracy\":max(resnet_shape_fit.history[\"val_categorical_accuracy\"]), \\\n",
    "        \"loss\":resnet_shape_fit.history['loss'], \"val_loss\":resnet_shape_fit.history['val_loss'], \\\n",
    "        \"accuracy\":resnet_shape_fit.history['categorical_accuracy'], \"val_accuracy\":resnet_shape_fit.history['val_categorical_accuracy']}\n",
    "    attempt += 1\n",
    "    print(\"For batch size {} and epochs of {}, performance:\".format(each[\"batch_size\"], each[\"epochs\"]))\n",
    "    plot_learning_curve(resnet_shape_fit.history['loss'], resnet_shape_fit.history['val_loss'], \n",
    "                    resnet_shape_fit.history['categorical_accuracy'], resnet_shape_fit.history['val_categorical_accuracy'], \n",
    "                    metric_name='Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_shape_saved_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(resnet_shape_saved_results)):\n",
    "    plot_title = \"Batch Size: {} Epochs: {} ResNet\".format(resnet_shape_saved_results[i][\"batch\"], resnet_shape_saved_results[i][\"epoch\"])\n",
    "    plot_learning_curve(resnet_shape_saved_results[i][\"loss\"], resnet_shape_saved_results[i][\"val_loss\"], \n",
    "                    resnet_shape_saved_results[i][\"accuracy\"], resnet_shape_saved_results[i][\"val_accuracy\"], \n",
    "                    metric_name='Accuracy', title=plot_title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further tuning -> anything we want to tune within the layers like accuracy or loss function etc., need to add them as variables we pass in in the init in the CNN class, then simply pass each into the CNNModel() line, and within the class, pass in as a variable as well as defining the default as used in the baseline model, with the different values overrriding the default. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ultimate Judgement - Sign-Shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ultimate Judgement - Sign-Type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deployment on Independent Data"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1af506060f0e02bd0a36060294fda6793d74ecacb3bb75555bc17d3e15d25309"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('tf_gpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
